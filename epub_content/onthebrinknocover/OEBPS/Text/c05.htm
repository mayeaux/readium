<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US">
<head>
  <title>On the Brink</title>
  <link href="../Styles/style.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <div class="chapter">
    <h1 class="chapter">V<br />
    Technology</h1>

    <p class="indent">There is yet another trend locked in exponential growth that, although we certainly have no issues witnessing and experiencing it, we hardly ever take the time to fully consider it. This trend is the exponential growth of computer technology—and technology in general. Perhaps throughout earlier periods of time in history, possibly even as far back as the 20th century, it would be more difficult to explain to those living on Earth that technology was growing upon itself at an exponential rate. When very little changes in the course of a generation and your access to information is limited, it would be very hard to piece together enough data to fully appreciate where we have come, or where we are going. The implications of exponentially growing technology may not be fully understood in a world where past generations only used livestock and crude machinery to live a life of agriculture. However, in today’s day and age, with today’s generation, it will be much easier to explain how technology has begun to take off at speeds literally never seen before by humans. I would contend that it would be difficult for someone to not have noticed this massively important trend, considering the way our entire lifestyles have changed in less than two decades. Think about the change that would have taken place between 5000 BC and 4980 BC; probably not too much to write home about. Now think about the change that has taken place just from 1993 to 2013; the world has changed completely, even a few times over. Now think about the change that will take place from 2013 to 2033; can we even comprehend it—quantify it—imagine it?</p>

    <p class="indent">The answer is yes. When we consider the scenario of change taking place in ancient times, versus the speed of change in our current electronic, mechanized world we can see that obviously some very powerful and world-altering effects are at work. It may even be difficult for someone during the early 20<sup class="frac">th</sup> century to be able to realize that technology is increasing at an exponential rate. They would have to really think about it to be able to understand that, although technology is certainly increasing, that the speed at which it is increasing it increasing as well. I do not think I will have as much difficulty driving home that point with modern day readers, when we can see it evidently taking place in our day to day lives. How long did it take for cell-phones to arrive on the scene? Then how long did it take for most people to own a cell-phone? And then how long did it take for high-speed wireless internet, in a phone which records 1080P videos, with computing speeds greater than 1970s computer which took up entire rooms—that also slides easily into a pants pocket? Yes, it certainly is easier to prove the reality of exponentially increasing technology in the modern day than it would be at any other time in history.</p>

    <p class="indent">We may be inclined to think that this exponential growth of technology began with the advent of the modern transistor computer in the 1970s, but that is not the case. This growth in technology is not a function of anything that we as humans have been up to. The inception of this story is actually found in physics, and its beginning is actually with the Big Bang (yes, we are going all the way back!)</p>

    <p class="indent">The trends at work that have been pushing technology forward, which we can now easily quantify with growth in computer power, actually long predate the entire human species. Many who study what I am going to document in this chapter will also tell you that the story not only begins long before humans, but will continue on long after humans (if you can see what they’re getting at). But before I can hit you with sexy, mindboggling statistics that make your eyes blink and your eyebrows cock, we have to lay down the intellectual framework. As always, simply being told that something will happen is just not good enough; we have to understand why, and understand it so well that we would be able to explain it to someone when explaining why they absolutely must purchase a copy of ‘On The Brink’. (Wink). With that being said, let’s start.</p>

    <p class="indent">What we are really experiencing when we see these huge leaps in computer technology is something that may at first seem physically impossible, which is the speeding up of time itself. Is that really possible? Does a second today not have the same value as a second experienced in say, our earlier example of 5000 BC? How can time itself be speeding up?</p>

    <div class="blockquote">
      <p class="center2">“Put your hand on a hot stove for a minute, and it seems like an hour. Sit with a pretty girl for an hour, and it seems like a minute. THAT’S relativity.”</p>

      <p class="center">Albert Einstein</p>
    </div>

    <p class="indent">Indeed, Einstein contended that time was relative to the individual who was experiencing it. According then to the laws of relativity, time can either speed up or slow down depending on who is experiencing exactly what. Now consider this: we used earlier the example of change happening in 5000 BC, and change happening in the modern day, and we could quite easily conclude that more significant events (events which altered the course of the future of the entire world) occurred between 1992 and 2012 than did between 5000 BC and 4980 BC. Science has beaten us to this topic a long time ago—we are not just an extremely clever author/reader combo which had noticed this idea. In physics, when examining increasing chaos or order over a period of time, we use the term <em>salient events</em> as the substitute for the phrase <em>significant events.</em> Salient events in this instance mean events that change the nature of the process, or significantly affect the future of the process (as this can be applied to more topics than just changing the course of the world).</p>

    <p class="indent">This trend of more events happening in a shorter period of time can be seen all the way back in the beginning of time on Earth, starting with the big bang. Before life on Earth, literally billions of years would pass, with the only effects taking place being the cooling of the planet and formation of electrons and positrons. I will add that the physical history of the world is certainly not my specialty—nor is physics in general, and in this regard I will simply be parroting the words of actual physicists whose books I have used in my research.</p>

    <p class="indent">Nonetheless, with the advent of the first organisms on Earth, evolution thereby became the major driving force of change in the world. However, this change began at a very slow, what we could consider a snail’s pace, when it was estimated that 3.4 billion years ago the first single-celled organisms had emerged. With the advent of DNA not seen in the first organisms on Earth, nature had created a code of sorts that could be manipulated; the sequences most suitable for their environment being those which were replicated. This was the engine that slowly gave way to the multitude of species now seen on Earth.</p>

    <p class="indent">This pace however, like mentioned before, was excruciatingly slow. While it took literally billions of years for the first single-celled organisms to appear on Earth, very soon it was only hundreds of millions of years that were necessary to witness major significant events occur. Beginning with the emergence of mammals, that time between salient events was again reduced, this time to just mere millions of years. By examining the DNA of lowland gorillas, whom we share 98.6% of our genetic code with, we can arrive at the conclusion that humans began to decouple from our fellow primates in terms of genetics about five hundred thousand years ago. This, juxtaposed with the past rate of billions of years, is indeed evidence of exponential growth. Consider then that industrial civilization has only been around for a few hundred years (~250). <em>That</em> is what exponential speeding up is all about. Just to finally drive home the point, Facebook was started only about eight years ago, and now it has over a billion active users. If we were to say that humans have been uniquely human for 500,000 years, then for 499,800 (or 99.96 %) of those years there were not even a billion people on the planet. Playing with the numbers is always fun, but we are only left with one conclusion: something very powerful is happening with this world. Luckily, we are part of the minority in the world which actually understand that this in fact is following a trajectory of the exponential speeding up of time.</p>

    <p class="indent">However evolution, if you can believe it, has been unseated from the throne of being the major driving force of today’s change. Evolution’s mandate to manipulate the structure of DNA and to work through the phenomenon of natural selection to let survive those best suited for the environment has been usurped by a new arrival on the scene. If evolution was still the king of the hill, we would be swinging from trees and changing the world slowly, one bout of sex at a time. Instead today, I am writing on a computer, connected to a network that will allow me to reach nearly every corner of the world. Something profound has certainly happened, the question is: if not evolution, then what?</p>

    <p class="indent">The answer is technology. By creating technology, our species has set the course of the world on a far different one than would be found if the world had only been able to rely on evolution to produce change. With technology, a system was created where change could take place much faster than by relying on the birth and death of life-forms. By innovating, and retaining those innovations by passing them on through generations, technology allowed humans to step outside the normal boundaries of nature, like we have seen with the example of oil and the limiting factors of population. If you think about the amount of change that technology presses forward with today, you can see that things can go from a new innovation to old news within a generation—something that evolution could simply not handle. This was certainly not the case with the advent of technology. When humans were simply sharpening stones—a very primitive form of technology—there was no force that would make last generation’s technology obsolete, like we have seen with the VHS, to DVD, to thumb drive. This is because like we have seen with evolution, technology is also increasing at an accelerating pace.</p>

    <p class="indent">When we were examining evolution, we found that first it took millions and even billions of years for substantial change to occur. Finally, when humans began to delineate from other primates, it was only hundreds of thousands of years. The time when <em>homo sapiens sapiens</em> (our species of human) emerged was only 90,000 years ago—now we are dealing in tens of thousands of years. Technology follows the same trend.</p>

    <p class="indent">Like I have mentioned before, it will not be hard to illustrate this point in today’s day and age. We have come a long way from sharpening stones—today a technology is groundbreaking for mere years before the newer and better thing comes out. It seems to be accelerating at a pace now that people, especially those in older generations, can barely hold on in keeping up with the new change. Who can blame them? It is not an uncommon sight to see a preteen child helping their parents or a teacher with figuring out how to operate certain functions on a computer. Again, these children have grown up around laptops, smartphones and wireless internet their entire life. Most parents can empathize more with walkmans and television, one which is all but extinct and another which is losing ground and quickly. Considering now that with an iPod in one pocket, a smartphone in another and a laptop in a backpack that a teenager could be carrying around three computers—it is difficult to believe that only 20 years ago, personal computers were large, expensive, bulky and slow. Their interfaces were difficult, they were unable to perform many useful functions for the everyday person and they were just simply uncommon. How can so much change happen in so little time? Well, it may not come as a surprise to you, but computer technology is but the latest technological trend to be pulled into exponential growth—with implications so severe and far-ranging that many have not even had time to consider the consequences of what is everyday moving closer to reality.</p>

    <p class="indent">This exponential increase in computing power is due to the <em>law of accelerating returns.</em> The law of accelerating returns is an easy way to think about exponential growth—something we are intimately knowledgeable about by now. Instead of just powering along at a steady, linear pace, within the law of accelerating returns, the growth (the returns) build upon themselves. Every new return speeds up the next one.</p>

    <p class="indent">When looking at the law of accelerating returns, we can apply it to first evolution, then human created technology, then computation all the way to Moore’s law, which we will examine shortly. These are all following exponential growth, which is just another way of describing accelerating return. As Kurzweil has stated, “technology is the continuation of evolution by other means” which as with evolution, continues to follow an exponentially quickening pace”.<a href="../Text/bib.htm#ft28" id="fn28"><sup class="frac">28</sup></a> It makes sense, too. If you were to look at the process of evolution, that is, due to death of organisms within a species, those with the traits most desirable within their necessities for survival will pass on their genetics, whereas those with the traits most poorly suited for their surroundings will die and become a genetic dead-end. What this process creates over the long term is the different species we see around us, that is: specialized and complex species that are designed specifically for life in their habitat.</p>

    <p class="indent">However, the slow dying of and rebirth of organisms is a process that takes a long time—it can only happen as quickly as a generation can pass. If we were to look at cell-phones from the same lens as we did evolution, we would see a similar process taking place but at a much, much more accelerated pace. Research In Motion, Apple and HTC all work to create, or give birth to different kinds of cell-phones. These cell-phones are developed, and then distributed across the world. As they begin to get ‘old’, wither and finally become obsolete, the newer, best functioning cell-phones of the new day sweep in to take their place. A new ‘generation’ of cell-phones can become antiquities after just a handful of years—the ‘evolution’ of cell-phones takes place at a speed much, much quicker than the biological evolution of humans. However, as opposed to humans, there are very easily quantifiable changes we can observe with the evolution of computers, that perhaps if they were following some set of pattern we could chart to look towards the future. It’s difficult to observe growth in intangible qualities such as human intelligence, but with the computing power of processors we run into no such snags—it is easy to observe data and see the trajectory that the power of computers are taking—and with that being said, we can follow that trajectory to some very profound conclusions.</p>

    <p class="indent">As Marion King Hubbert was the figure whose work exemplified the reality of peak production of oil reserves, Gordon Moore is the figure whose keen observation and ultimately prophetic prediction has come to define the reality of <em>exponentially</em> increasing power of computer technology. What Moore, who was a chairman for Intel in 1965, realized was that the size a transistor would take on a computer chip was decreasing by 50% every 12 months. This means that the amount of maximum transistors that could be fit on a computer chip would <em>double</em> every single year. Later he would update his figure to every 24 months, which is the trajectory that has been followed since 1975.</p>

    <div class="figure">
      <p class="center"><img alt="image" src="../Images/118-1.jpg" /></p>

      <p class="caption">Source: Wikipedia</p>
    </div>

    <p class="indent">What this means is that, as we’ve seen in this book many times already, this trend is now in the hands of exponential growth, one of the most powerful forms of growth we can imagine. Just like with human population, debt expansion and resource depletion—just knowing that this trend is in the hands of exponential growth allows the reader’s imagination to come to conclusions already. But before we start breaking down what a future in exponential growth of computer technology may look like in 10, 15 or 20 years, let’s first continue examining this exponential growth so we can say, emphatically, that <em>yes, we have proof of its happening.</em></p>

    <p class="indent">With that being said, the main thing to understand is that this exponential growth of computing technology did not begin with Moore’s observation in the mid 1960s, but it has in fact been following this exponential rate since as early as the very beginning of the 20<sup class="frac">th</sup> century (1900) with the advent of the Analytical Engine Mechanical Computing Device<a href="../Text/bib.htm#ft29" id="fn29"><sup class="frac">29</sup></a> (Kurzweil 22). In terms of bits that can be processed in a given amount of time, this rapid expansion has followed an exponential curve all the way through Vacuum-tube computers, through discrete transistor computers and finally through integrated circuit computers, which are now the standard. By charting the amount of calculations made per second per $1000 in a logarithmic graph, we can see that the trend since 1900 has been above all an exponential trend.</p>

    <div class="figure">
      <p class="center"><img alt="image" src="../Images/120-1.jpg" /></p>

      <p class="caption">Source: Kurzweil <em><span class="underline">The Age Of Spiritual Machines</span></em></p>
    </div>

    <p class="indent">So when we look at the large picture, all the way back to the first life on Earth, what can we see? Since the advent of the first single-celled organisms, time has essentially been slowing down in the objective sense, although this feels in the subjective sense (i.e. in the sense that we experience reality) that time is in fact speeding up. What we can see is that, beginning with evolution (which took at first billions of years to create change) a trailing off has occurred where change then only required millions of years to bring about substantive differences, continuing onto only hundreds of thousands and then mere tens of thousands of years between <em>salient</em> ‘entire process’ changing events. We can thus observe a steady trend of change occurring at a readily accelerating pace. Again: this is the change that was being brought about by nature’s own engine for change, evolution, via natural selection. However, evolution then created something which said rather optimistically “we’ll take it from here evolution”—that being us; the human species. With the advent of human created technology, a new trend usurped evolution from being the king of change. Although evolution certainly did not cease to exist, the constant <em>accelerating</em> returns of technology then became the major driving force of change.</p>

    <p class="indent">Whereas evolution would require the birth and deaths of generations to impart change, technology offered an ‘evolution by other means’, a much quicker and even more effective method which could propel the world forward. This trend of ‘technological evolution’ if you will, like evolution, followed and continues to follow an exponentially quickening pace. It would be hard to argue that human created technology has changed the world more than evolution, especially considering that evolution <em>technically</em> created technology itself, simply using humans as a vessel. However, it is impossible to deny the drastic effects that technology has brought to the world, and you may be justified in saying that these effects are more severe than evolution could have ever brought about.</p>

    <p class="indent">Continuing down this path through the course of the universe, eventually something was created by humans that could potentially dethrone humans as the pre-eminent dominant force in the world.</p>

    <p class="indent">Just as through evolution, something more powerful than evolution itself was created, through humans something more intelligent and powerful than themselves (ourselves) were created. With the advent of the digital computer, computers now posses a digital code that work analogously to our version of a digital code or DNA. Just as the first organisms on Earth lacked DNA which could be manipulated and stored genetic information, the first computers too lacked binary code. However, that is (of course) no longer the case. Could it be that humans are simply a stepping stone in the universe’s march towards greater order and complexity? Are we the be-all end-all, or are we simply going to be passing the torch—and if so, to whom, and when?</p>

    <p class="indent">Don’t worry. We will get to that. First, let’s lay down more groundwork.</p>

    <p class="indent">It’s important to understand that Moore’s law of exponentially growing amounts of transistors which can fit on a chip is by no means the only figure surrounding computer technology that is moving at an exponential rate. Not even close. Moore’s law is simply <em>one of</em> the ways in which digital computers are increasing in power and efficiency. The fact is, just like we can look at the exponential growth of many things in our world: be it human population, fisheries depleted, forests deforested, even paper clips in existence (you can get away with using your imagination here)—in the same manner that it would be improper to reference the amount of paper-clips in the world as the major driving force in the world, it would be improper to look at the amount of transistors on a chip as the major driving force behind computer technology. The truth is that all of these things operate in lockstep with the underlying exponential slowing down of time itself. The real story is the exponential equation itself, not the quantifiable things we can point at and say “hey look!” We can look towards things such as number of calculations per second per dollar, pixels per dollars in cameras or hard disk capacities. Witnessing these trajectories is extremely important because it allows us to make projections for our capabilities in the future.</p>

    <div class="figure">
      <p class="center"><img alt="image" src="../Images/123-1.jpg" /></p>

      <p class="caption">Source: Kodak</p>
    </div>

    <div class="figure">
      <p class="center"><img alt="image" src="../Images/123-2.jpg" /></p>

      <p class="caption">Hard Drive Capacities, Source: Wikipedia</p>
    </div>

    <p class="indent">Moore’s law continues to chug along to this day, however it too will come to an end. Just like all its predecessors, it will reach a conclusion. Just like vacuum-tube computers eventually reached their physical peak, and a new technology came along to carry on the torch of exponential growth, transistor chip computers will not be the vessel in which we reach the peak of computing power. Based on data, it was pegged at around the year of 2020 in which Moore’s law would cease to exist. And actually, there is some funny (depending on your sense of humour) news about exactly that.</p>

    <p class="indent">“The smallest transistor ever built -- in fact, the smallest transistor that can be built -- has been created using a single phosphorus atom by an international team of researchers at the University of New South Wales, Purdue University and the University of Melbourne.” In a journal published February 19<sup class="frac">th</sup> of 2012, it was shown that researchers have now created a transistor that is one atom thick—as thin as transistors can physically ever be. This, it may appear, signals that the end of Moore’s law has already occurred in 2012, but Moore’s law is not a measure of <em>technology existing</em>, but a measure of technology that has already hit the market, available to be purchased. Considering that the market availability of this technology usually lags around eight to ten years, it would appear that the prediction of Kurzweil and others, of a 2020 end for Moore’s law seems to be continuing right on target. A poignant quote from one of the researchers was found in the article.</p>

    <p class="indent">“Fifty years ago when the first transistor was developed, no one could have predicted the role that computers would play in our society today. As we transition to atomic-scale devices, we are now entering a new paradigm where quantum mechanics promises a similar technological disruption. It is the promise of this future technology that makes this present development so exciting.” The researchers would then go onto say that “this is the physical limit of Moore’s law. We can’t make it smaller than this”. <a href="../Text/bib.htm#ft30" id="fn30"><sup class="frac">30</sup></a></p>

    <p class="indent">Which is true, but like I have mentioned before, something will eventually supersede Moore’s law—exactly what we cannot yet tell, but the exponential growth <em>will</em> continue. Just like it would have been historically impossible to anticipate the creation of transistor chip computers, whatever that technology will look like is simply beyond our grasp at this moment. However, like the researcher has said: a new paradigm will be entered.</p>

    <p class="indent">And this is where we are left today: on the brink of amazing technology, technologies that we literally cannot yet comprehend, or even imagine. All we know is that this technology, specifically trends surrounding computer technology, will continue to move at a rate that is <em>exponential</em>. I know this word has been ponied around quite a lot in this book—of course, considering that it is the theme, but still keep in mind exactly what is meant when <em>exponential growth</em> is referenced. Consider this quote from Kurzweil:</p>

    <p class="indent">“Exponential trends are immensely powerful but deceptive. They linger for eons with very little effect. But once they reach the “knee of the curve,” they explode with unrelenting fury. With regard to computer technology and its impact on human society, that knee is approaching with the new millennium” – Ray Kurzweil</p>

    <p class="indent">In context of this quote: we are, right now, in the “knee of the curve”. This ‘unrelenting fury’ referenced by Kurzweil is today felt in terms of smartphones, whose computing power is literally tens of thousands of times more powerful than that of former computer which would take up entire wings of universities—smartphones which also have 1080P cameras, are connected to a nation-wide wireless internet network, and are in the hands of people who grew up rewinding VHSs and blowing in the cartridges of Nintendo Games. So if we are sitting on the cusp, like it has been posited by technologists (researchers and scientists), then what does our immediate future look like? Perhaps five years ago when I had first come across the ideas of Moore’s law and the exponential growth of computing power, it would be hard to feel out where we were headed. Someone could explain the concepts of nanotechnology: computers so small they were the size of blood cells, which could be put into someone’s blood stream to carry out a manner of jobs—we could hear about these concepts, but couldn’t quite see what they would look like. Considering however the advancements that have happened in just these short five years, we can already see amazing technologies that are poking through from the future and already making a scene here in the present day. In brief, most of the trouble of imagining the future has already passed: it’s arrived.</p>

    <p class="indent">Consider for example, the Turing test. Alan Turing was the father of modern day computing—one of the most brilliant mathematical minds that have ever existed, and I don’t think that point can be argued. He nearly single-handedly pioneered the concepts and creation of modern computing devices, and even programmed these rough, initial computers to be able to crack Nazi codes in the Second World War. Though he committed suicide, after being persecuted for being gay, his legacy in the field of computing still lives on, and in one of the most distinct ways being his contemplations surrounding artificial intelligence.</p>

    <p class="indent">In contemplating what creates distinction between human consciousness, and a machine produced consciousness, Alan Turing devised a system that has generally been regarded as the real test of true artificial intelligence. As the paradox goes: if a machine can be made to say hello, to look sad, or to ‘express emotions’—is it really expressing emotions, or is it just expressing an algorithm, the programming that has created this machine. Of course, this leads to many different opinions, including the idea that “perhaps we as humans are just machines ourselves, and express emotions based only on the programming within us”. This has been a topic of deep speculation for many who consider what consciousness truly is, but Alan Turing went a step further and created a system by which, if certain conditions were met, we would be forced to conclude that finally artificial intelligence was true intelligence.</p>

    <p class="indent">This system is called the Turing Test. In the Turing Test, digital computers and humans alike both participate in what is called ‘The Imitation Game’. In this game, an ‘interrogator’—a human whose job is to try and decide whether or not he is talking to a computer or a human -- asks of his counterpart a handful of questions. Rather, they engage in a dialogue. After a set amount of time, the interrogator then has to decide whether or not they were conversing with an ‘artificially’ intelligent computer, or rather if they were speaking to a fellow human.</p>

    <p class="indent">Mind you, the Turing Test is highly subjective, and the parameters surrounding it can always be adjusted. It is not without its criticisms, but it also serves as a great landmark. So, the question is, how far along are we? With Turing having introduced this idea in a 1950 paper called <em>‘Computing Machinery and Intelligence’</em>, how close have we come in the following 63 years?</p>

    <p class="indent">The answer may be surprising. Artificial intelligence has already passed the Turing Test.</p>

    <p class="indent">Now before you jolt out of your chair or break into a nervous sweat, consider this: there are disagreements and squabbles amongst the scientific community about what truly qualifies artificial computer intelligence as ‘intelligent’. Some would argue that the Imitation Game is simply not the correct way in which to test for intelligence in an artificial capacity.</p>

    <p class="indent">My purpose in writing this is not to analyze the ways in which the Turing Test could be conducted, or improved upon, or whether or not it is the correct method to test for artificial intelligence. I am simply here to tell a story of exponential growth in computer power, and right now I am just giving a rundown on where we are <em>at this moment.</em> And now that you know and understand what a Turing Test is, let’s examine what may be the first ever passing of the Turing Test, and you as the reader can decide whether or not this is acceptable to you as the first signs of artificial intelligence.</p>

    <p class="indent">Regardless of your conclusion however, it should be realized that there is a knocking on the door, and that the lines will continue to blur ever more. I imagine that as more sophisticated ‘chatbots’ as they are called enter the scene, they will spur increasingly greater amounts of heated debate about what being human really is, what consciousness really is, and whether or not a machine can actually ‘think’. Regardless, the point should be taken that this is no longer an armchair theorizing, or a distant speculation, like it was in centuries past when the idea was first considered. These upcoming qualms, along with the other facts in this book are yet more realities that will need to be faced by humans. Without building up the suspense any longer, let’s examine what may arguably be the first passing of the Turing test by artificial intelligence.</p>

    <p class="indent">The name of the program is Cleverbot. Cleverbot is a chatbot who with a clever bit of design is able to learn responses to questions and statements. Essentially, if you were to log on to the website at <a href="http://cleverbot.com">cleverbot.com</a> and began to talk with Cleverbot, it would keep a careful note of what you have been saying. This is how Cleverbot is able to learn so quickly (currently there are over 10,000 people online ‘talking’ to Cleverbot at this moment). Through this, Cleverbot has learned proper syntax (sentence formation) and is also able to recall and make reference to things that have already happened in the conversation (saying for example: “you’ve already asked me that”). Sound simple enough? Well, consider this:</p>

    <p class="indent">In a formal Turing Test that was conducted in September of 2011, Cleverbot went processor-to-toe with 100 humans at the Techniche 2011 festival in India in vying for a spot amongst human conversations. Thirty members of the audience were chosen to participate, and chatted electronically by text in ten 4 minute rounds (for a total of 40 minutes). Half of the conversations were human-human, and after each conversation the ‘interrogator’ was to pick, on a scale of 1-10, how ‘human’ they thought the person they were talking to was.</p>

    <p class="indent">Now, what is a ‘pass’ in the Turing Test? Is it like it was in public education—anything over 50 is a pass and anything under 50 is a fail? If so, then we can say that Cleverbot has in fact passed the Turing Test, based on the fact that out of the 1334 conversations, it was decided that <em>Cleverbot was 59% human</em>. It may be interesting to compare this to the fact that humans themselves were decided to be only 63% human. Thus, we were left, in September of 2011, with a -4% difference between the ability of humans to differentiate between whether not they were speaking to a programmed chatbot, or in fact another human.</p>

    <p class="indent">When I hopped on the website, <a href="http://www.cleverbot.com">http://www.cleverbot.com</a>, I figured I would go straight to the source and ask Cleverbot myself some questions. Mind you, the only Cleverbot that we are ever able to access through the internet is the very ‘Lite’ version of Cleverbot. The one that was run in the test in June used much more processing power and ran much deeper algorithms (and was thus ‘more human’). It’s not difficult to understand why, considering there were a total of 10759 others ‘talking to Cleverbot’ at that same moment (that is a lot of learning!) This is how the conversation went, so you can have a bit of judgment. Would you be able to tell that this is in fact a chatbot and not in reality another human?</p>

    <div class="blockquote">
      <p class="block">Anthony: So, are you human or what?</p>

      <p class="block">Cleverbot: Yes, I am human.</p>

      <p class="block">Anthony: Have you truly passed the Turing Test?</p>

      <p class="block">Cleverbot: What is the Turing Test?</p>

      <p class="block">Anthony: A test conducted by Alan Turing to test for artificial intelligence. You’re going to have to get a copy of my book.</p>

      <p class="block">Cleverbot: Ah. No, I have not. Since I am not ‘Artificial Intelligence’, it would be awkward taking such a test as one designed for me to pass.’</p>
    </div>

    <p class="indent">So we already know where Cleverbot stands on the issue of whether or not it is a human. However, it is important to note that Cleverbot is simply parroting what it has been told to him by others humans. (Think for a second though—isn’t that just what all of us do?) Nonetheless, this can lead to some inaccuracies, at least in the online version of Cleverbot. Considering that the bot is usually participating in upwards of 10,000 conversations at a time, we only ever get a truly watered down version of Cleverbot, due to simply the amount of computing power and server sizes it would need to service the requests of over 10,000 people simultaneously. With what we know about the exponential growth of computing power though, the door is left wide open for what Chatbots can achieve in the near future.</p>

    <div class="blockquote">
      <p class="block">“When I was a student at MIT, we all shared one computer and it took up a whole building. The computer in your cell phone today is a million times cheaper and a thousand times more powerful. What now fits in your pocket 25 years from now will fit into a blood cell and will again be millions of times more cost effective.” –Ray Kurzweil</p>
    </div>

    <p class="indent">As we’ve seen with our examination of Moore’s law: technology is not only getting more powerful at an exponential rate, but it’s getting smaller as well. It’s already been described how scientists are now manipulating transistors on the atomic level, and although it will be many years until the public at large has access to these technologies, we can already begin to piece together some images of how the future may look in a world where, like Kurzweil says in the above quote, technology can merge with blood cells and manipulate the world at an atomic level (far beyond the sight of the naked human eye).</p>

    <p class="indent">What this technology is called is <em>nanotechnology</em>. If you can imagine, let’s say we have to put together a table. We need the table top, and the four legs that we’ll screw in place for the top to stand on. Now let’s say we were screwing this together all by hand—IKEA style. We would have no trouble gripping the legs, and screwing them in. But let’s say we wanted a miniature table, this table has all the same dimensions, but rather it is scaled down ten times, to one tenth the original size—grabbing onto those table legs now becomes that much more difficult.</p>

    <p class="indent">Now let’s scale it down ten more times, until the table is so small it would fit easily onto the palm of your hand. Another ten times scaling down—now this table can easily fit on the end of your thumb. With that being said, how difficult now would it be to take that table and manually screw the table legs in with just your (all of a sudden very fat) thumbs. This is exactly what is meant by nanotechnology, this is what Kurzweil is trying to capture when he explains that a single computer used to take up multiple rooms in a university, and was 1/1000<sup class="frac">th</sup> as powerful as the <em>consumer phone</em> which when I bought it took me only a couple days of work to afford.</p>

    <p class="indent">The other thing to realize is that this isn’t a table that’s shrinking in size—it’s computer technology. A miniature table that fits onto your thumb—cool. A miniature computer that merges with your blood cells to make them more efficient—now you’re talking. Of course, for all of human history up until these most recent years, the technology simply did not exist to allow humans to manipulate matter atom by atom. But with the law of accelerating returns in technology, we have been hurtling with dare I say atomic speed towards the day when we can finally create devices so tiny they make our fingernail sized table look clumsy and obtuse.</p>

    <p class="indent">The possibilities of this technology are extremely broad. For example, one thing it will certainly allow us to do is create materials that are a hundred fold more powerful than conventional materials. It has even been speculated that using these materials will allow us to create the once hypothetical ‘elevator to the moon scenario’.<a href="../Text/bib.htm#ft31" id="fn31"><sup class="frac">31</sup></a> Truly, nanotechnology will remain as one of the least understood yet most important developments of the coming years. In fact, self-replicating nanotechnology could possibly run out of control and consume the entire Earth, a scenario which has been called ‘grey goo’, a term coined by Eric Drexler, an American nanotechnology engineer. Even depending on the point at which you read these words, this technology could have taken off and transformed into something not quite yet feasible as I write this on the eve of 2013. This is opposed to the dissolution of the separation of man and machine, which is well underway even as we come to face the New Year.</p>

    <p class="indent">The line between human and robotics continues to be blurred at an increasing rate with every passing day. This is one of the main reasons I have chosen to include a section on technology in this book, because what once may have been fun speculation and imagination is now becoming modern day reality.</p>

    <p class="indent">Whereas once we could sit back and debate ethics surrounding artificial technology, or allowing robots to kill humans, today these topics are still being discussed while the technology is already beginning to emerge. There are still many ethical questions that have to be answered. Considering the truly incomprehensible amount of power that control of this new technology will grant, who will we allow to possess it—will there be regulations in place, and if so then what, and to be enforced by who?</p>

    <p class="indent">To many kept in the mainstream lull of the latest distraction material (celebrity gossip usually works best), these questions I doubt will ever ‘happen to cross their mind’. I am truly sceptical that many who live within the matrix of the corporate produced mainstream are even aware of exponential growth: what it really is, or the fact that it is currently acting on such things as human population and the power of computer technology. If this technology is created, controlled and distributed by only the utmost echelon of society (those who have effectively compromised and co-opted modern day democracy) then the policies surrounding these technologies will reflect only the wishes of that very sliver of society, while placing the much larger majority (the 99.99%) in a very precarious situation.</p>

    <p class="indent">What happens when the day comes which will allow the super-rich to create brain implants that boost their intelligence by tenfold? What if a ruthless corporation monopolizes nanotechnology? Or, in a trend that is already very alive and real, what happens when highly militaristic states with a distaste for human life begin using these Supertechnologies to impose their will on other nations? Of course, this is already happening with ‘Predator drones’, which are flown from remote locations in the United States and already have a presence in Afghanistan Pakistan, Yemen, Somalia <a href="../Text/bib.htm#ft32" id="fn32"><sup class="frac">32</sup></a> and are now being deployed in the United States itself. <a href="../Text/bib.htm#ft33" id="fn33"><sup class="frac">33</sup></a> What if I told you too that already these Predator drones are <em>choosing by themselves</em> whether or not to fire on a target, effectively neutralizing any human input. <a href="../Text/bib.htm#ft34" id="fn34"><sup class="frac">34</sup></a> Yes—that is already a reality in the present day, computerized machines which attack from far above what the human eye can detect, which base their decisions on whether or not to ultimately fire based on an algorithm, written (in the meantime) by a human. I hope you can already begin to see the range of issues that this raises.</p>

    <p class="indent">Another distinct area which it will be very important to draw definite boundaries within is something which was alluded to earlier: that being the integration of computerized technology into the actual human body. This, like much present day technology, blurs many ethical lines and puts directly into our faces the question: what is it to be human?</p>

    <p class="indent">This is a recurring theme in the section on modern technology, and it draws many philosophical quagmires into question. However, this modest book does not by any means attempt to draw the conclusions for the reader: it is up for each person to consider themselves the ramifications of embracing certain technologies, and then for them to make a personal decision themselves. The sooner the better too, as there can be no doubt that there is now a timeline we are now working on, and if the general population does not consider and come to conclusions on these deep, philosophical questions, then it will be left to the people who are already at the forefront in pressing these technologies forward. Being how it is with the power of the corporate state to bend political will around their deep and vast resources (chequebook), it is certainly not unfounded to have a worry that these corporations will not consider the ecological nor humanitarian effect on the general population, but will instead, as is their mandate (and their only reason for existing) try to maximize profits in all possible venues, by any means necessary. Certainly there needs to be an understanding amongst the general population of the true power of these technologies, lest we let Halliburton or other faceless, heartless and ruthless corporate powers drive forward the human agenda. Now back to the technology itself.</p>

    <p class="indent">There is, currently, a group of people who are well aware of the exponential growth of computer technology and are positioning themselves to be able to take full effect of this power when it becomes available (examples to come). When examining this exponential plot of computing power, all the way from the analytical machines of the early 1900s, to the vacuum tube computers, we can see quite distinctly that we are now entering ‘the knee of the bend’, or in other terms, we are currently sitting in the spot in time where things start to really heat up. We are getting pulled out of this growth, that if you zoomed out far enough would look flat for thousands of years, and into a growth which will soon be propelled straight up in the air like a rocket. This portion of the exponential curve, when we are catapulted vertically, has been colloquially termed a ‘singularity’, a meaning which took its name from the space-time singularity of crossing through a black hole.</p>

    <p class="indent">Now, like there are those who are well aware of peak oil and preparing appropriately, and like there are those who are well aware of the inevitable collapse of our monetary system and also preparing, there are similarly those who are astutely aware of the coming technological singularity and doing much of the same (if I have done my job correctly by the end of this book you will fall into all three categories).</p>

    <p class="indent">What these who are preparing for the technological singularity are doing is biding their time, eagerly watching the technology explode and waiting for the right moment. These people have already done all of the philosophical consideration, they have spent perhaps sleepless nights contemplating how they will move through the future, and they have reached a firm conclusion. They will happily open themselves up to this technology in any venue. To them, there is nothing ‘distinct’ or special about being human, to them a human is merely a machine that was designed by evolution—who was a very smart craftsman but certainly did not finish the job. We run too slow, we can’t jump very high, we can’t remember things very well, nor calculate things very well, all of which are seen as shortcomings to this particular group of people. I will do my best to leave my personal opinion completely out of the following narrative, with admittance that I myself have not reached the philosophical grounds that will allow to firmly stake myself on one side of the fence or the other. Thus, I will (like I have been attempting to do for most of this book), leave my personal thoughts out of the fore completely and instead just deliver the information and facts.</p>

    <p class="indent">Nonetheless, to these people the human body and mind is a very incomplete creation and should be either tinkered with or outright manipulated. They are thus very open to adopting this new technology: integrating it within themselves and embracing it with open arms. This line of thought, which happily embraces integrating the abiological with the biological belong to a school of thought called <em>transhumanism.</em> These people will not wince nor mince words when they say that they are happily waiting for the day that they can cash out of their faulty eyesight and hearing and instead replace it with mechanized computer eyesight and hearing. They are happy to call themselves cyborgs. And if in fact it seems too farfetched for us to begin talking about human cyborgs, just know this: they already exist, and realistically, they are not too difficult to find these days.</p>

    <p class="indent">When Rob Spence lost his eye in a shooting accident, he chose to do something a little out of the ordinary, and rather than just simply have a glass eye put in, he worked with a team of engineers to construct a rather human looking eye, which was in fact a camera, which could record seamlessly, and without pause, uploading the data to an external server where it could be saved forever. Therefore, this person has a complete data set of everything he sees, he is recording his life perpetually. What he has (and he shies not away from this description) is a robotic eye, and what he is (and he is quite happy to wear this description) is one of Earth’s first cyborgs.</p>

    <p class="indent">Would you be able to, with an ear to ear grin, call yourself a cyborg as done this person? You may think not, but you may also be surprised by the willingness in which so many of those who are aware of the coming singularity are impatiently waiting to do that very thing, imagining romantically about the possibilities of the future, where nanotechnology could make all of their dreams come true.</p>

    <p class="indent">Perhaps it is not so unbelievable, in a world where people will inject massive amount of anabolic steroids to gain an edge, or wear platform heels or surgically repair their eyes; humans are always looking to gain the upper hand. We already have computers that hardly leave our arms reach which can record 1080P video and transmit data across the world, with the internet, GPS or phone towers. Is it so unnatural for some to want to merely take a step to integrate that into their bodies, rather than carry it in a jeans pocket? Some will say yes, certainly: humans are humans and we should stay that way. Some will look at those people as luddites, and chastise them for standing in the way of science and technology. I imagine the showdown between these quarrelling factions will continue for as long as we continue to develop these technologies, but it will ultimately be up to each person to decide for themselves where they stand with these issues.</p>

    <p class="indent">A specific topic that is being addressed as quickly as possible in the tech community is the physical divide between humans and their technology. We have the internet in our pockets, yes, but to access it we need to reach into our pockets, pull it out, and then occupy our limbs to maneuver and operate the machinery. Much like this would be simply the talk of science fiction only a few years ago, yet is today increasingly becoming a material reality, Google is hard at work producing a set of eye-wear that functions with an HUD or heads up display. By projecting images directly onto the human retina, and using sensors that can detect where the eye is looking, the final steps are being taken in finally removing the manual input of fingers in directing our technology. The vision Google has, a joy for both techies and authoritative regimes, is that of a world where simply by looking at certain queues on a digitally displayed system, you can query your glasses and get feedback on where your friend is in terms of arriving to meet you for a cup of coffee.</p>

    <p class="indent">Google will of course be integrating the bounding increases in the functionality of voice and speech recognition. As we have seen on a mass scale now with Apple’s ‘Siri’ (a version of a chatbot), those who are analyzing future technology anticipate in the near future the ability to be able to work seamlessly with computerized helpers who assist in your day to day living. These personal assistants are not far away from creation and many tech companies are working tirelessly to make them a normal part of everyday living.</p>

    <p class="indent">Another very important philosophical issue to deal with is the emergence of artificial technology. A topic that has been touched briefly in our discussion of Cleverbot, the day when artificial intelligence arrives that seems indeed extremely ‘human’ is upon us. Google again is driving home this trend, with the creation of what are literal ‘artificial brains’. This was done by creating a ‘neural network’ (i.e. a brain) by connecting 16,000 computer processors (which we already are aware are shrinking and becoming more effective at an exponential rate) with one billion neural connections and began to expose it to 10 million randomly selected thumbnails of YouTube videos. After this neural network, or artificial brain, was shown these 10 million thumbnails, it was then tested on its ability to distinguish things such as human faces, human body parts and cat faces.</p>

    <p class="indent">Now think about this. The researchers in this ‘experiment’ of sorts were not instructing this artificial brain on what a cat was and what a cat wasn’t. It wasn’t shown a cat in the beginning of the instructions, nor afterwards. Picture this even from the point of view of a child: if you are to show a child 10,000 pictures, with no instructions whatsoever, and then they are shown a picture of a cat’s face (perhaps even at an obscure angle or a face that is somewhat difficult to distinguish)—what chances do you think the child has of correctly pointing out and thinking to itself ‘hey—I know what that is?’. Keep in mind, this child (like this artificial brain) has no idea what the concept of a ‘cat’ is—they are merely recognizing a pattern.</p>

    <p class="indent">Do you have a percent probability that you think you could show a small infant and have them recognize the fact that they are looking at this familiar pattern of a cat’s face? With absolutely no instruction, this artificial brain was able to distinguish cats with a 74.8% accuracy. It was able to distinguish human faces with an 81.7% accuracy. To take a pessimistic Terminator-Skynet point of view, just to illustrate the point in an easily understood manner (who can’t understand or hasn’t contemplated machine take overs?) this means that 81.7% of the time this machine is recognizing that you are in fact a human and thus (if it is programmed for this nature) must be killed. All hyperbolic end of the world scenarios aside, it’s majorly impressive.</p>

    <p class="indent">We can look at graphs of the amount of neurons within the brain of certain animals, and fit it onto an exponential plot. It starts with creatures such as snails and slugs, and moves through mammals, and we can plot the human brain about halfway up the ‘knee of the curve’. These symbolize neurons remember, which are basically brain cells, which when connections are formed via the synapses, create things such as memory, higher processing functions (in primates) and even personality. Humans have an estimated 80 billion neurons within the brain. That is 80,000,000,000—and that’s extremely impressive. Evolution has certainly done a knockup job in creating biological intelligence (and if you think about it, each of our cells is really a self-replicating nanobot). Without straying too far off the point, consider this, a snail possesses about 11,000 neurons in its brain cell. <a href="../Text/bib.htm#ft35" id="fn35"><sup class="frac">35</sup></a> Now, we can replicate these brain cells using connections with computers, and it may come as no surprise that the amount of cells that we can replicate have been increasing at an exponential rate. Thus, we have been able to set up synthetic brains as powerful as snails, and slugs, and these kinds of creatures, and we have been steadily working our way up the curve all the way to humans. Where are we now? Well, the artificial brain that Google has just created that taught itself how to identify cats and humans has 1,000,000,000 ‘neurons’, or 1/80<sup class="frac">th</sup> of that in a human brain. <a href="../Text/bib.htm#ft36" id="fn36"><sup class="frac">36</sup></a></p>

    <p class="indent">So, if computer technology will continue to expand at an exponential rate, and we have already whizzed by smaller creatures’ brains, will we ever reach the power of a human brain? The answer is: yes, and by all calculations, 2020 seems to be about the date. The real questions from that point on, questions that I imagine no one has an answer to, are: what happens when that date is reached—what will the brain look like, be able to do, and function as? And the real question is: what happens from that point on; what happens when the human brain has its date with destiny and is ultimately surpassed in pure computing power by artificial intelligence?</p>

    <p class="indent">Of course, there has been a drive since long before we could produce digital computers to create a robotic creature in our likeness. Amongst those who currently produce humanoids, that is human robots, there is a deep desire to create a being that can operate seamlessly in communication with humans. That is, they can look like a human, speak and have facial expressions like a human, and even have thoughts that are human. David Hanson, CEO of Hanson Robotics and a man on the forefront of creating social, humanoid robots always recommends interacting with robots as if they were human, because it is his belief that through social interactions is the only way we can create a ‘human’ robot.</p>

    <p class="indent">Which leaves us with this: if there are, as this book is being read, an uncounted amount of engineers, scientists, aestheticians, and generally competent people, all working towards creating a machine human that can pass for an actual human, the real question is how far are we to reaching this collective goal? Well the answer is that we are already extremely close.</p>

    <p class="indent">There is a theory called the ‘uncanny valley’ theory which basically posits that if a human robot is <em>almost</em> perfect, but certain things cause the humans to recognize easily that it isn’t a robot (for example, rigidity in the movement of its limbs or poor synchronization), that there will be a ‘valley’ created in terms of human comfort. That is, if you were to plot a graph of human comfort as a function of how human the robot appears, there would be a large dip directly before the robot becomes almost perfectly human. In a metalloid, very apparently machine robot, there is less of an ‘uncanny’ feeling produced. But if you can imagine, staring at a robot that looks exactly human in facial structure and body, and then it makes very weird eye motions and its mouth is not synchronized with its words, you would definitely be feeling uncanny. I feel uncanny just thinking about it.</p>

    <p class="indent">However, this is very bad for roboticist David Hanson, who as we have mentioned before is looking to create robots that <em>are human</em> by interacting with them in a human way. If someone is repulsed by the feelings that are produced within themselves by watching a nearly human robot act in very odd, nonhuman ways, then the likelihood that these humans will interact with the robot in an empathetic, human way is very slim. Thus, we can imagine the uncanny valley as a hill that is first climbed as human beings begin to act more empathetic as blatantly machine-like robots start to become more and more human, but then begins to nose-dive as these machines begin to approach human likeness. At the very bottom of this valley, we can imagine a robot that is freaking a ton of people out because it looks quite human but has certain odd features or mannerisms that are really putting people off. That is at the very bottom—from that point all there is to do is climb as these mannerisms are slowly (or quickly) chipped away at until we reach a point where it is impossible to discern between a robotic human and a biological human.</p>

    <p class="indent">Based on research I have done looking at models of humanoids produced in the years since 2007, I would have to conclude that we are past the bottom of the peak and already working on the way up. However, this is only in terms of robots from the neck-up. The technology surrounding facial expressions made by humanoids, and their ability to engage in a dialogue leads me to believe that the worst, most ‘revulsion producing’ near-human faces have already been past in and around 2008.</p>

    <p class="indent">If artificial intelligence, humanoid robots and the likes are all news to you, then you will be relieved to know that there have not yet been produced humanoids that can walk and make bodily gestures as effortlessly and routinely as humans. Although if they are stationary, and wearing a lot of clothing, humanoid robots can make gestures in their speech similar to a human, we are still a ways from creating a robot that can walk with the same elegance as a human being. Something that appears very simple to us, such as walking up stairs, requires for robots an insane amount of articulation and calculations.</p>

    <p class="indent">Certainly, the other effect that arises from studying the growth of humanoid robots, besides an eerie wonder of what a fully metal machine ‘human’ can actually achieve, is a deep amazement for what the human body and brain is able to do, considering there are over 7,000,000,000 (7 billion) of us existing on this planet right now. Our cells are able to construct and organize themselves in a way we can only dream nanotechnology will be able to do, our brains are able to calculate trajectories, judge facial expressions, remember faces, and our bodies able to effortlessly maneuver steps, jumps, even faces of mountains.</p>

    <p class="indent">When Garry Kasparov, arguably the greatest chess player of all-time, played against IBM’s Deep Blue it was simply a man in a chair making his calculations, versus a machine that literally took up several entire rooms and cost $5 million dollars to create (this was in 1996). One can only wonder how much more electricity it took to run the brain of Deep Blue, versus the brain of Garry Kasparov, who was running on simply meals and water. For the time being, the human body and mind remain as the greatest machine on Earth.</p>

    <p class="indent">However, the quest to tweak the human body and mind, to receive an edge, a leg up on the competition, has always been a fundamental staple of human desire. As we have seen with Rob Spence when he lost his eye, and we can see from the now fully bionic arms that people receive instead of rudimentary ‘hook-like’ artificial limbs, people today losing a limb or an organ gives them the possibility to cash out of the biological and into something more powerful (if you can fork up the cash). Where does this leave us headed for the future, in a world of depleting resources, debt and economy collapses, with the added chaos of exponentially increasing power of computing technology? This is the long-awaited discussion for the final chapter of the book.</p>
  </div>
</body>
</html>
